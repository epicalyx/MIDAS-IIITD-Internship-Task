{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the task of classifying emotions from speech modality. Emotion classification involving multiple modalities like text, speech and facial expressions are on a high rise now owing to several reasons like:\n",
    "- The developments made in machine learning techniques that offer great solutions to such tasks.\n",
    "- Due to the rise in social media and networking that has given researchers vast amont of data to experiment and build models on.\n",
    "- Due to the wide applications the emotion classication task has, ranging from communication between differently abled, to providing personalized customer care support.\n",
    "\n",
    "Here, we will focus on the speech aspect only. Simply put, we are given an audio clip, and our job is to classify it according to the emotional state it corresponds to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context and Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion is an important aspect of communication. Although the field of emotion classification is not new, there is still a lot of left to be explored and achieved. This is because emotions are ambiguous and therefore complex to be modelled. Emotions can be misunderstood even by humans, and therefore the task of even labelling the dataset for speech emotion recognition, is cumbersome and challenging, unlike the task of maybe annotating image datasets for computer vision problems.\n",
    "Next, we need features to work on, researching on which features to use and then crafting them is again a challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset being used here is: **MELD - A Multimodal Multi-Party Dataset**.\n",
    "- It is available at: https://github.com/SenticNet/MELD\n",
    "- Research paper for the same: https://arxiv.org/pdf/1810.02508.pdf\n",
    "\n",
    "The dataset has audio files in \".wav\" format and there are 5 emotion classes, namely:\n",
    "1. Disgust\n",
    "2. Fear\n",
    "3. Happy\n",
    "4. Neutral\n",
    "5. Sad\n",
    "\n",
    "We have two folders in this- one for training set and other for validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly 2 model architectures have been explored in this notebook:\n",
    "\n",
    "### Model 1\n",
    "\n",
    "**Multi Layer Percetron**\n",
    "\n",
    "Acoustic features represented in the form of MFCC and MEL(discussed later under \"Feature Extraction\") of an audio wave have been extracted using the polular library used for audio analysis - **Librosa**. \n",
    "The features are then passed into a fully connected artificial neural network with 300 units in the hidden layer.\n",
    "\n",
    "### Model 2\n",
    "\n",
    "**Convolutional Neural Network**\n",
    "\n",
    "The same features extracted are reshaped to be fed into a regular CNN, with 6 layers for feature extraction and one output layer for classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Accuracy| \n",
    "|--|--|\n",
    "|Multi Layer Perceptron|55.66%|\n",
    "|Regular Convolutional Neural Network|62.29%|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing General Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for mathematical computations on matrices\n",
    "import matplotlib.pyplot as plt # for visualising data and results\n",
    "import pandas as pd # to work with dataframes\n",
    "import zipfile \n",
    "import random \n",
    "import sys\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import soundfile # to read audio file\n",
    "import librosa # to extract speech features\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split # for splitting training and testing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier # multi-layer perceptron model\n",
    "from sklearn.metrics import accuracy_score # to measure how good we are\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.models import model_from_json # to save and write trained model in json format\n",
    "import json\n",
    "import pickle # to save model after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducible results\n",
    "random.seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract contents from zip file, uncomment following lines\n",
    "#zip_ref = zipfile.ZipFile('emotion_final.zip', 'r')\n",
    "#zip_ref.extractall()\n",
    "#zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are given audio files in \".wav\" format. \n",
    "#### Considering the complexity of the task, rule based approaches wouldn't be a good choice, and therefore we are going with machine learning approaches\n",
    "#### Following a machine learning pipeline, our first goal is to extract features from these files, that can then be modelled using various machine learning/deep learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Feature Extraction\n",
    "#### In order to extract relevant features, we first must know what features are present in an audio. \n",
    "#### Information on emotion is encoded in all aspects of language, in \"what\" we say and in \"how\" we say, and the \"how\" part is definitely more important than the \"what\" part. The way we speak comes under acoustic features of the spoken language and what we speak comes under context based features. \n",
    "#### Now, depending upon what we are trying to achieve, we should choose the features to be extracted. \n",
    "#### For example, if we are to classify emotions from speech, then acoustic features should be an obvious choice since our emotions directly influence the way we speak in terms of our pitch, intensity and voice quality. On the other hand, if are to perform speech emotion recognition or more appropriately, modelling of complex emotions through speech, which can't be put into definite classes, then we shall depend upon contextual features as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are to classify emotions, and thus we will go with acoustic features. Some of the relevant and widely used acoustic features are:\n",
    "1. Mel-Spectogram Frequency, which are representations of the short-term power spectrum of a sound.\n",
    "2. MFCC (Mel-frequency cepstral coefficients); these coefficients collectively make up MFC.\n",
    "\n",
    "#### We will be using Librosa, a popular library for audio analysis and its feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            \n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X,sample_rate = librosa.load(file_name, sr=16000)\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now apply the feature extraction process to the dataset to obtain feature vectors and their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(data_path: str,\n",
    "             class_labels: Tuple = (\"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\")):\n",
    "    \"\"\"Extract data for training and testing.\n",
    "    1. Iterate through all the folders.\n",
    "    2. Read the audio files in each folder.\n",
    "    3. Extract Mel frequency cepestral coefficients for each file.\n",
    "    4. Generate feature vector for the audio files as required.\n",
    "    Args:\n",
    "        data_path (str): path to the data set folder\n",
    "        class_labels (tuple): class labels that we care about.\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray, numpy.ndarray]: Two numpy arrays, one with mfcc and\n",
    "        other with labels, and a submission file having audio file names which will later be appended with model predictions\n",
    "    \"\"\"\n",
    "    signal = []\n",
    "    labels = []\n",
    "    names = []\n",
    "    cur_dir = os.getcwd()\n",
    "    sys.stderr.write('curdir: %s\\n' % cur_dir)\n",
    "    os.chdir(data_path)\n",
    "    for i, directory in enumerate(class_labels):\n",
    "        sys.stderr.write(\"started reading folder %s\\n\" % directory)\n",
    "        os.chdir(directory)\n",
    "        for filename in os.listdir('.'):\n",
    "            filepath = os.getcwd() + '/' + filename\n",
    "            feature_vector = extract_feature(filepath,mfcc = True, mel = True)\n",
    "            signal.append(feature_vector)\n",
    "            labels.append(directory)\n",
    "            names.append(filename)\n",
    "        sys.stderr.write(\"ended reading folder %s\\n\" % directory)\n",
    "        os.chdir('..')\n",
    "        submission_file = pd.DataFrame(names,columns = ['File name'], index = None)\n",
    "    os.chdir(cur_dir)\n",
    "    return np.array(signal), (labels),submission_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "curdir: C:\\Users\\sande\\meld_final\\train\\disgust\n",
      "started reading folder disgust\n",
      "ended reading folder disgust\n",
      "started reading folder fear\n",
      "ended reading folder fear\n",
      "started reading folder happy\n",
      "ended reading folder happy\n",
      "started reading folder neutral\n",
      "ended reading folder neutral\n",
      "started reading folder sad\n",
      "ended reading folder sad\n"
     ]
    }
   ],
   "source": [
    "# obtaining training data to be fed into models \n",
    "X_train,y_train, train_file = get_data(data_path = r\"C:\\Users\\sande\\meld_final\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "curdir: C:\\Users\\sande\\meld_final\\train\\disgust\n",
      "started reading folder disgust\n",
      "ended reading folder disgust\n",
      "started reading folder fear\n",
      "ended reading folder fear\n",
      "started reading folder happy\n",
      "ended reading folder happy\n",
      "started reading folder neutral\n",
      "ended reading folder neutral\n",
      "started reading folder sad\n",
      "ended reading folder sad\n"
     ]
    }
   ],
   "source": [
    "# obtaining validation data to be tested upon\n",
    "X_val,y_val,val_file = get_data(data_path = r\"C:\\Users\\sande\\meld_final\\val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check for class imbalancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>lables</th>\n",
       "      <th>lable</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MEL_dia1000_utt0_negative_DIS.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MEL_dia1000_utt1_negative_DIS.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MEL_dia1005_utt13_negative_DIS.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MEL_dia1005_utt21_negative_DIS.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MEL_dia1005_utt6_negative_DIS.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            File name   lables    lable    label\n",
       "0   MEL_dia1000_utt0_negative_DIS.wav  disgust  disgust  disgust\n",
       "1   MEL_dia1000_utt1_negative_DIS.wav  disgust  disgust  disgust\n",
       "2  MEL_dia1005_utt13_negative_DIS.wav  disgust  disgust  disgust\n",
       "3  MEL_dia1005_utt21_negative_DIS.wav  disgust  disgust  disgust\n",
       "4   MEL_dia1005_utt6_negative_DIS.wav  disgust  disgust  disgust"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file['label'] = y_train\n",
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x161dc351390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAFBRJREFUeJzt3X20ZXV93/H3BwYkFnkwjJYwxBmTiQk0imQKJNrGgIUBomAKLozViSGly2Jj2qymY5Yu69MK5onGVEkmATuSVAJqHJSkZBZgXZoojIA8SCgjgo6ojB2elCIOfvvH3uMcLvfMPXcczr6X3/u11l1779/+nXu/56x772c//PbeqSokSe3Za+gCJEnDMAAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjVoydAG7csghh9Ty5cuHLkOSFpXPfe5z36yqpXP1W9ABsHz5cjZt2jR0GZK0qCS5e5J+HgKSpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNWpBXwi2Jyxfe8XQJQBw13mnDl2CJD2OewCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNWriAEiyd5Ibkny8X16R5LNJ7kjyV0n27duf1i9v7tcvH/keb+rbb09y0p5+M5Kkyc1nD+CNwG0jy+8Gzq+qlcB9wNl9+9nAfVX148D5fT+SHAGcBRwJrAbel2TvH6x8SdLumigAkiwDTgX+vF8OcDzwob7LeuD0fv60fpl+/Ql9/9OAS6rqO1X1JWAzcMyeeBOSpPmbdA/gvwG/BXyvX/5h4P6q2t4vbwEO6+cPA74C0K9/oO///fZZXvN9Sc5JsinJpq1bt87jrUiS5mPOAEjyi8C9VfW50eZZutYc63b1mp0NVeuqalVVrVq6dOlc5UmSdtOSCfq8CHh5klOA/YAD6PYIDkqypN/KXwbc0/ffAhwObEmyBDgQ2DbSvsPoayRJUzbnHkBVvamqllXVcrqTuFdX1auBa4Az+m5rgA39/OX9Mv36q6uq+vaz+lFCK4CVwLV77J1IkuZlkj2Acf4LcEmSdwI3ABf27RcCFyfZTLflfxZAVd2a5FLgC8B24NyqeuwH+PmSpB/AvAKgqj4BfKKfv5NZRvFU1SPAmWNe/y7gXfMtUpK053klsCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaNWcAJNkvybVJPp/k1iRv69tXJPlskjuS/FWSffv2p/XLm/v1y0e+15v69tuTnPRkvSlJ0twm2QP4DnB8Vb0AOApYneQ44N3A+VW1ErgPOLvvfzZwX1X9OHB+348kRwBnAUcCq4H3Jdl7T74ZSdLk5gyA6nyrX9yn/yrgeOBDfft64PR+/rR+mX79CUnSt19SVd+pqi8Bm4Fj9si7kCTN20TnAJLsneRG4F5gI/BF4P6q2t532QIc1s8fBnwFoF//APDDo+2zvGb0Z52TZFOSTVu3bp3/O5IkTWSiAKiqx6rqKGAZ3Vb7T83WrZ9mzLpx7TN/1rqqWlVVq5YuXTpJeZKk3TCvUUBVdT/wCeA44KAkS/pVy4B7+vktwOEA/foDgW2j7bO8RpI0ZZOMAlqa5KB+/oeAlwK3AdcAZ/Td1gAb+vnL+2X69VdXVfXtZ/WjhFYAK4Fr99QbkSTNz5K5u3AosL4fsbMXcGlVfTzJF4BLkrwTuAG4sO9/IXBxks10W/5nAVTVrUkuBb4AbAfOrarH9uzbkSRNas4AqKqbgBfO0n4ns4ziqapHgDPHfK93Ae+af5mSpD3NK4ElqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1Kg5AyDJ4UmuSXJbkluTvLFvf2aSjUnu6KcH9+1J8p4km5PclOToke+1pu9/R5I1T97bkiTNZZI9gO3Ab1bVTwHHAecmOQJYC1xVVSuBq/plgJOBlf3XOcAF0AUG8FbgWOAY4K07QkOSNH1zBkBVfa2qru/nHwJuAw4DTgPW993WA6f386cBH6jOZ4CDkhwKnARsrKptVXUfsBFYvUffjSRpYvM6B5BkOfBC4LPAs6vqa9CFBPCsvtthwFdGXralbxvXPvNnnJNkU5JNW7dunU95kqR5mDgAkuwPfBj4jap6cFddZ2mrXbQ/vqFqXVWtqqpVS5cunbQ8SdI8TRQASfah++f/l1X1kb75G/2hHfrpvX37FuDwkZcvA+7ZRbskaQCTjAIKcCFwW1X94ciqy4EdI3nWABtG2l/bjwY6DnigP0R0JXBikoP7k78n9m2SpAEsmaDPi4DXADcnubFv+23gPODSJGcDXwbO7Nf9DXAKsBl4GHgdQFVtS/IO4Lq+39uratseeReSpHmbMwCq6lPMfvwe4IRZ+hdw7pjvdRFw0XwKlCQ9ObwSWJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNmuRuoHqKWL72iqFLAOCu804dugRJuAcgSc0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqPmDIAkFyW5N8ktI23PTLIxyR399OC+PUnek2RzkpuSHD3ymjV9/zuSrHly3o4kaVKT7AH8D2D1jLa1wFVVtRK4ql8GOBlY2X+dA1wAXWAAbwWOBY4B3rojNCRJw5gzAKrqk8C2Gc2nAev7+fXA6SPtH6jOZ4CDkhwKnARsrKptVXUfsJEnhookaYp29xzAs6vqawD99Fl9+2HAV0b6benbxrU/QZJzkmxKsmnr1q27WZ4kaS57+iRwZmmrXbQ/sbFqXVWtqqpVS5cu3aPFSZJ22t0A+EZ/aId+em/fvgU4fKTfMuCeXbRLkgayuwFwObBjJM8aYMNI+2v70UDHAQ/0h4iuBE5McnB/8vfEvk2SNJAlc3VI8kHgJcAhSbbQjeY5D7g0ydnAl4Ez++5/A5wCbAYeBl4HUFXbkrwDuK7v9/aqmnliWZI0RXMGQFW9asyqE2bpW8C5Y77PRcBF86pOkvSk8UpgSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqPmvBBMeipavvaKoUvgrvNOHboENc49AElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcqHwkuNW772iqFL4K7zTh26hCYZAJLUay0MPQQkSY0yACSpUQaAJDXKAJCkRhkAktSoqQdAktVJbk+yOcnaaf98SVJnqgGQZG/gvcDJwBHAq5IcMc0aJEmdae8BHANsrqo7q+pR4BLgtCnXIEkCUlXT+2HJGcDqqvq1fvk1wLFV9YaRPucA5/SLzwNun1qB4x0CfHPoIhYIP4ud/Cx28rPYaSF8Fs+pqqVzdZr2lcCZpe1xCVRV64B10ylnMkk2VdWqoetYCPwsdvKz2MnPYqfF9FlM+xDQFuDwkeVlwD1TrkGSxPQD4DpgZZIVSfYFzgIun3INkiSmfAioqrYneQNwJbA3cFFV3TrNGnbTgjokNTA/i538LHbys9hp0XwWUz0JLElaOLwSWJIaZQBIUqMMAElqlAEgSY0yADSnJG9IcvDQdWjhSLJikranuiR7Jbll6Dp2l88EniHJQ8y4OnnHKqCq6oApl7QQ/FPguiTXAxcBV1ZDw8eS3MzsvxMAVNXzp1jOQvFh4OgZbR8CfmaAWgZTVd9L8vkkP1pVXx66nvkyAGaoqmcMXcNCU1VvTvIW4ETgdcB/T3IpcGFVfXHY6qbiF/vpuf304n76auDh6ZcznCQ/CRwJHJjkl0ZWHQDsN0xVgzsUuDXJtcC3dzRW1cuHK2kyBsAckjyLkV/sxZjye0JVVZKvA18HtgMHAx9KsrGqfmvY6p5cVXU3QJIXVdWLRlatTfJp4O3DVDaI59EF4kHAy0baHwL+7SAVDe9tQxewu7wQbIwkLwf+APgR4F7gOcBtVXXkoIUNIMmvA2vo7nD458BHq+q7SfYC7qiqHxu0wClJciPwhqr6VL/8c8D7quqoYSubviQ/W1X/MHQd+sF4Eni8dwDHAf+nqlYAJwCfHrakwRwC/FJVnVRVl1XVd6E7/snOwyMtOBt4b5K7ktwFvA/41WFLGswrkhyQZJ8kVyX5ZpJ/M3RRQ0hyXJLrknwryaNJHkvy4NB1TcI9gDF23NI1yeeBF/Yne66tqmOGrm0ISY4GXkx3MvTTVXX9wCUNJskBdH87Dwxdy1CS3FhVRyV5BXA68B+Ba6rqBQOXNnVJNtHd2PIyYBXwWmBlVf32oIVNwHMA492fZH/gk8BfJrmX7th3c/oTwK8EPtI3vT/JZVX1zgHLGkSSU+lOgu6XdI+3qKqWzgHssE8/PQX4YFVt2/F5tKiqNifZu6oeo/v7+Puha5qEATDeacD/o9uyeTVwIG2d7Bv1y3R7QY8AJDkPuB5oKgCS/AnwdOAX6M6FnAFcO2hRw/lYkn+k+xv590mWAo8MXNNQHu5vb39jkt8Fvgb8k4FrmoiHgGbRP7z+yqp66dC1LARJ/hZ4VVXd3y8fBPxFVbV0/J8kN1XV80em+wMfqaoTh65tCP3FgQ9W1WNJng4cUFVfH7quaUvyHOAbwL50G4wH0g0O2DxoYRNwD2AW/S/0w0kObPk474jv0I1z3kh3DuBfAZ9K8h6Aqvr1IYuboh1buA8n+RFgG9Dc1a8ASV47Mj+66gPTr2ZYVXV3kh8CDq2qRTUk1AAY7xHg5v6f3ujFHa38sxv11/3XDp8YqI6hfazf+/k9ukNgBfzZsCUN5p+PzO9HN0ruehoMgCQvA36fbg9gRZKjgLd7IdjidkX/NarJ42VVtb4/xvmTdJ/B7VX16MBlDeEfgceq6sNJjqC7FcJHB65pEFX1H0aXkxzIziukW/NfgWPoN4yq6sYky4crZ3IGwHgHVdUfjTYkeeNQxQwpySnAnwJfpLsn0ook/66q/nbYyqbuLVV1WZIX0x0G+wPgAuDYYctaEB4GVg5dxEC2V9UDi3EUlBeCjbdmlrZfmXYRC8QfAr9QVS+pqp+nGwVz/sA1DeGxfnoq8CdVtYFut785ST6W5PL+6+PA7cCGoesayC1JfhnYO8nKJH8MOAx0MUryKrphjyuSXD6y6hnA/x2mqsHdO2NEw510t8dozVeT/CnwUuDdSZ5GuxtRvz8yvx24u6q2DFXMEJJcXFWvodszPpJusMQHgSvp7iSw4DkMdIZ+SNcK4HeAtSOrHgJuqqrmLgZLcgHdvZAupTsHcCbdFt+nAarqI+Nf/dTRD3VcDdxcVXckORT46ar6u4FL0wCSfAE4Gbicbq/4capq29SLmicDQHNK8v5drK6qavV+OM0a89yMB4BNwG9W1Z3Tr2q6+pskvh54LvDV0VV0fxfPHaSweTAAxpjxC74v3aXv3270gTDS4yR5G3AP8D/p/uGdRffgoNuB11fVS4arbrqSXFBVrx+6jt1hAEwoyenAMYvhBk97WpL96O6EeSSPfzaCW/6NSvLZqjp2Rttnquq4JJ9v8aZwi1GrJ7Dmrao+Chw/dB0DuZhu6+4k4H8Dy+jOiahd30vyyv6ZuHsleeXIOrcqFwn3AMaY8bi7vehu8/rzVfWzA5U0mCQ3VNULR+6Bsw/dvZJaDcTmJXku8EfAjr+Hf6C7D85XgZ/Z8dAcLWwOAx1v9HF324G76O4Q2qLv9tP7k/wzusdCLh+uHA2tP8n7sjGr/ee/SBgAY1TV64auYQFZ19/58c10Q972B94ybEkaUn/b43fS3Q76fwEvAH6jqv5i0MI0L54DGCPJT/SPurulX35+kjcPXddALqYb7/xiYD3wXuDZg1akoZ1YVQ/SPRJ0C/ATwH8etiTNlwEw3p8Bb6I//FFVN9ENdWvRBrrDX9uBb/Vf397lK/RU94Qngg1ZjHaPh4DGe3pVXTvjBk/NXQXcW1ZVq4cuQguKTwR7CnAPYLxvJvkx+iFtSc6ge9Rbi/4+yU8PXYQWjqpaSzcCaFVVfZduj7DVQRKLlsNAx+iHua0Dfg64D/gS8OqqunvQwqYoyc10AbiE7la/d9Ld8GrHpe7PH7A8DSDJ8VV19Yxh0t/Xyn2hnio8BDTeV4H3A9cAzwQepLtFdEsPhm/qmb+ayL8ErqYbAlr0GwMjUwNgETEAxtsA3E/3mLt7Bq5lEC3t7WhiDyX5T8At7PzHD179uygZAON54lN6ov376fPongu8gS4EXgZ8cqiitHs8BzBGknXAH1fVzUPXIi00Sf4O+NdV9VC//AzgMjeaFhf3AMZ7MfArSb6EJz6lmX4UeHRk+VG8PciiYwCMd/LQBUgL2MXAtUn+mu74/yvorhLXIuIhIEm7JcnRwL/oFz9ZVTcMWY/mzwCQpEZ5JbAkNcoAkKRGGQCS1CgDQJIa9f8BwEGC5v7q3z0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_file['label'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The plot above indicates high class imbalancy. Let's see how this might affect our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Number of training samples: 7354\n",
      "[+] Number of testing samples: 830\n",
      "[+] Number of features: 168\n"
     ]
    }
   ],
   "source": [
    "# print some details\n",
    "# number of samples in training data\n",
    "print(\"[+] Number of training samples:\", X_train.shape[0])\n",
    "# number of samples in testing data\n",
    "print(\"[+] Number of testing samples:\", X_val.shape[0])\n",
    "# number of features used\n",
    "# this is a vector of features extracted \n",
    "# using extract_features() function\n",
    "print(\"[+] Number of features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a classification problem, let's first try with a basic Artificial Neural Network(MLP).\n",
    "\n",
    "**Model Architecture 1**\n",
    "\n",
    "The network discussed below is a fully connected (dense) neural network with one hidden layer containing 300 units, a batch size of 128, trained for 500 iterations using an adaptive learning rate. These are by far the best set of hyperparameters I experimented with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following are the parameters for the MLP Classifier'''\n",
    "\n",
    "model_params = {\n",
    "    'alpha': 0.01,\n",
    "    'batch_size': 128,\n",
    "    'epsilon': 1e-08, \n",
    "    'hidden_layer_sizes': (300,), \n",
    "    'learning_rate': 'adaptive', \n",
    "    'max_iter': 500, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size=128, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.66%\n"
     ]
    }
   ],
   "source": [
    "# model predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_true=y_val, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's save our model for future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"result_ser\"):\n",
    "    os.mkdir(\"result_ser\")\n",
    "pickle.dump(model, open(\"result_ser/mlp_classifier.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing submission files\n",
    "submission_mlp = val_file\n",
    "submission_cnn = val_file\n",
    "submission_lstm = val_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>meld_final/val\\disgust\\MEL_dia15_utt7_negative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>meld_final/val\\disgust\\MEL_dia17_utt12_negativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>meld_final/val\\disgust\\MEL_dia17_utt14_negativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>meld_final/val\\disgust\\MEL_dia17_utt3_negative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>meld_final/val\\disgust\\MEL_dia17_utt4_negative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File name\n",
       "0  meld_final/val\\disgust\\MEL_dia15_utt7_negative...\n",
       "1  meld_final/val\\disgust\\MEL_dia17_utt12_negativ...\n",
       "2  meld_final/val\\disgust\\MEL_dia17_utt14_negativ...\n",
       "3  meld_final/val\\disgust\\MEL_dia17_utt3_negative...\n",
       "4  meld_final/val\\disgust\\MEL_dia17_utt4_negative..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_mlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending predicted labels for the validation set\n",
    "submission_mlp['prediction'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>meld_final/val\\sad\\MEL_dia8_utt0_negative_SAD.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>meld_final/val\\sad\\MEL_dia9_utt11_negative_SAD...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>meld_final/val\\sad\\MEL_dia9_utt2_negative_SAD.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>meld_final/val\\sad\\MEL_dia9_utt8_negative_SAD.wav</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>meld_final/val\\sad\\MEL_dia9_utt9_negative_SAD.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             File name prediction\n",
       "825  meld_final/val\\sad\\MEL_dia8_utt0_negative_SAD.wav    neutral\n",
       "826  meld_final/val\\sad\\MEL_dia9_utt11_negative_SAD...    neutral\n",
       "827  meld_final/val\\sad\\MEL_dia9_utt2_negative_SAD.wav    neutral\n",
       "828  meld_final/val\\sad\\MEL_dia9_utt8_negative_SAD.wav        sad\n",
       "829  meld_final/val\\sad\\MEL_dia9_utt9_negative_SAD.wav    neutral"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_mlp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving submission file\n",
    "submission_mlp.to_csv(r'C:\\Users\\sande\\submission_mlp.txt', header=None, index=None, sep=',', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second approach will be a Convolutional Neural Network**\n",
    "#### Why?\n",
    "Convolutional Neural Networks have shown promising results for both image classification and audio classification. This is because the embeddings used by CNN classifiers work much better than the audio features extracted Since we are to label a given audio into discrete set pf emotion classes, according to acoustic features, CNN seem to be a good choice.\n",
    "On the other hand if we were to model arousal and valence features of a given speech, that is perform context analysis, we should choose a Recurrent Neural Network (LSTM).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing dataset to be trained and tested**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the categorical labels using Label Encoder, so that it can be fed into CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_val = np_utils.to_categorical(lb.fit_transform(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7354, 168)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping feature vectors from 2 dimension to 3 dimensional, so that they can be fed into CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_valcnn= np.expand_dims(X_val, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Building\n",
    "#### Model Architecture 2\n",
    "A 6 layer architechture has been proposed, which has 4 Convolutional 1-D layers, a dropout layer and a maxpooling layer. These feature extraction layers are then followed by a Dense output layer used for final classification of emeotions. \n",
    "\n",
    "**Activation Function** - ReLU has been used in feature extraction layers and softmax for the output classification layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(168,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 168, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 168, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 168, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 168, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 168, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 13445     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 343,045\n",
      "Trainable params: 343,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sande\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7354 samples, validate on 830 samples\n",
      "Epoch 1/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0864 - acc: 0.6138 - val_loss: 1.0785 - val_acc: 0.6229\n",
      "Epoch 2/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0634 - acc: 0.6244 - val_loss: 1.0666 - val_acc: 0.6229\n",
      "Epoch 3/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0546 - acc: 0.6247 - val_loss: 1.0767 - val_acc: 0.6229\n",
      "Epoch 4/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0515 - acc: 0.6246 - val_loss: 1.0713 - val_acc: 0.6229\n",
      "Epoch 5/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0489 - acc: 0.6247 - val_loss: 1.0763 - val_acc: 0.6229\n",
      "Epoch 6/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0469 - acc: 0.6244 - val_loss: 1.0740 - val_acc: 0.6229\n",
      "Epoch 7/7\n",
      "7354/7354 [==============================] - 19s 3ms/step - loss: 1.0458 - acc: 0.6254 - val_loss: 1.0702 - val_acc: 0.6229\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=7, validation_data=(x_valcnn, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affect of hyperparameter tuning in this case**\n",
    "\n",
    "I tried several sets of hyperparameters for this, and the one included above seems to perform better than others.\n",
    "\n",
    "Since there is a major issue with the features extracted and problem of imbalanced classes too, the role of different hyperparameters is negligible. \n",
    "Still, I observed the following trends with learnig rate and weight decay:\n",
    "\n",
    "- On increasing the learning rate, the accuracy increases, though only a little.\n",
    "- Decreasing the learning rate hurts the performance.\n",
    "\n",
    "- Increasing weight decay produces slightly better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing training and test set losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving model weights, to directly use them for predicting in future**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:\\Users\\sande\\saved_models\\Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model 2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "acc: 62.29%\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Voice_Detection_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_valcnn, y_val, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# model predictions\n",
    "y_pred = loaded_model.predict(x_valcnn, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02174705, 0.02462263, 0.13787486, 0.6255542 , 0.19020122],\n",
       "       [0.03818088, 0.03727229, 0.18855684, 0.59321785, 0.1427722 ],\n",
       "       [0.03141043, 0.03374623, 0.16974618, 0.6352713 , 0.12982588],\n",
       "       ...,\n",
       "       [0.03239993, 0.03393225, 0.17729552, 0.62399495, 0.13237736],\n",
       "       [0.027522  , 0.0300059 , 0.15475681, 0.64678425, 0.1409311 ],\n",
       "       [0.03351827, 0.03703905, 0.17496392, 0.63138956, 0.12308923]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds=y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting encoded labels back into categorical type\n",
    "y_preds = lb.inverse_transform(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending predicted labels to the file\n",
    "submission_cnn['prediction'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving submission file\n",
    "submission_cnn.to_csv(r'C:\\Users\\sande\\submission_cnn.txt', header=None, index=None, sep=',', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't get a very promising result from either of the models discussed. Now, there could be mainly two reasons for that:\n",
    "\n",
    "1. High class imbalancy, we can see that are around **4500** samples with **neutral** emotion while only about **200** in the **fear class**. This affects the features that are being extracted and hence the overall performance and results of our models.\n",
    "\n",
    "2. I tried the above two models on different datasets available for speech emotion classification, namely **EMODb(Berlin Database for speech)** and **RAVDESS** datasets. The models seem to work quite well on these two datasets. Therefore, we can suspect that there is an issue with the features extracted from the audio files of **MELD**. This is also the reason for slow convergence of our models(CNN in particular)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. My next goal for this particular task, will be to try different features extraction techniques (hand crafted and otherwise), and therefore improvise upon this step. \n",
    "2. Another workaround will be to to solve class imbalancy issue and try different model architectures \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several failed attempts:\n",
    "**Below are few of my attemps for the same task, which didn't work out well**\n",
    "1. I reproduced the results for the MELD dataset discussed in this [repository](https://github.com/SenticNet/conv-emotion/tree/master/DialogueRNN), which explores a novel architecture called Dialogue RNN. I didn't include it here, because it wasn't really the solution I could come up with on my own!\n",
    "2. I tried the model discussed in this [repository](https://github.com/harry-7/speech-emotion-recognition) too, but the model wasn't converging at all!\n",
    "3. I tried the model discussed [here](https://github.com/maelfabien/Multimodal-Emotion-Recognition/tree/master/01-Audio). I couldn't observe it for long, since it is quite a heavy model to be run even with GPU support. I will be trying it out once again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspirational Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few useful resources that really helped me understand the whole pipeline of a speech emotion classification task:\n",
    "1. https://github.com/maelfabien/Multimodal-Emotion-Recognition/tree/master/01-Audio\n",
    "2. https://www.thepythoncode.com/article/building-a-speech-emotion-recognizer-using-sklearn\n",
    "3. https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer\n",
    "4. https://github.com/harry-7/speech-emotion-recognition\n",
    "\n",
    "Amazing research papers to read for this domain:\n",
    "1. https://github.com/maelfabien/Multimodal-Emotion-Recognition/blob/master/01-Audio/Resources/Speech%20Emotion%20Recognition%20using%20Convolutional%20Neural%20Networks.pdf\n",
    "2. https://arxiv.org/pdf/1810.04635v1.pdf\n",
    "3. https://arxiv.org/pdf/1811.00405.pdf\n",
    "4. https://arxiv.org/pdf/1810.02508.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
